# Serverless deployment optimized for RunPod FlashBoot (500ms-2s cold start)
# Multi-stage build for minimal image size
ARG BASE_IMAGE=h200-mug-positioning:base-latest
FROM ${BASE_IMAGE} AS base

FROM base AS builder

# Switch to root for installation
USER root

# Copy source code
COPY --chown=h200user:h200user src/ /app/src/
COPY --chown=h200user:h200user configs/ /app/configs/
COPY --chown=h200user:h200user docker/preload_models.py /app/preload_models.py

# Pre-compile Python files for faster startup
RUN python -m compileall -b /app/src && \
    find /app/src -name "*.py" -delete

# Create model preload script for FlashBoot optimization (backup inline version)
RUN cat > /app/preload_models_backup.py << 'EOF'
import os
import sys
import torch
import clip
from ultralytics import YOLO
import asyncio
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def preload_models():
    """Preload models into memory for FlashBoot optimization"""
    try:
        # Set device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {device}")
        
        # Load YOLOv8 model
        logger.info("Loading YOLOv8 model...")
        yolo_model = YOLO('yolov8n.pt')
        yolo_model.to(device)
        
        # Load CLIP model
        logger.info("Loading CLIP model...")
        clip_model, preprocess = clip.load("ViT-B/32", device=device)
        
        # Warm up models with dummy inference
        logger.info("Warming up models...")
        dummy_image = torch.randn(1, 3, 640, 640).to(device)
        _ = yolo_model.predict(source=dummy_image, verbose=False)
        
        dummy_clip_input = torch.randn(1, 3, 224, 224).to(device)
        with torch.no_grad():
            _ = clip_model.encode_image(dummy_clip_input)
        
        logger.info("Model preloading completed successfully")
        return True
    except Exception as e:
        logger.error(f"Model preloading failed: {e}")
        return False

if __name__ == "__main__":
    asyncio.run(preload_models())
EOF

# Switch back to non-root user
USER h200user

# RunPod serverless handler
RUN cat > /app/handler.py << 'EOF'
import os
import sys
import json
import base64
import asyncio
import logging
from typing import Dict, Any, Optional

# Add src to path
sys.path.insert(0, '/app')

# Import core modules
from src.core.analyzer import H200ImageAnalyzer
from src.database.mongodb import MongoDBClient
from src.database.redis_client import RedisClient
from src.database.r2_storage import R2Storage
from src.utils.secrets import SecretManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global instances for connection pooling
analyzer: Optional[H200ImageAnalyzer] = None
mongodb: Optional[MongoDBClient] = None
redis: Optional[RedisClient] = None
r2: Optional[R2Storage] = None

async def initialize_services():
    """Initialize all services with connection pooling"""
    global analyzer, mongodb, redis, r2
    
    try:
        # Initialize database connections
        mongodb = MongoDBClient()
        redis = RedisClient()
        r2 = R2Storage()
        
        # Initialize analyzer
        analyzer = H200ImageAnalyzer(
            mongodb_client=mongodb,
            redis_client=redis,
            r2_client=r2
        )
        
        # Ensure connections are established
        await mongodb.connect()
        await redis.connect()
        
        logger.info("All services initialized successfully")
        return True
    except Exception as e:
        logger.error(f"Service initialization failed: {e}")
        return False

async def process_request(job: Dict[str, Any]) -> Dict[str, Any]:
    """Process RunPod serverless request"""
    try:
        # Extract input
        input_data = job.get("input", {})
        
        # Get image data (base64 or URL)
        image_data = input_data.get("image")
        image_url = input_data.get("image_url")
        
        if not image_data and not image_url:
            return {
                "error": "No image data or URL provided",
                "status": "failed"
            }
        
        # Process image
        if image_data:
            # Decode base64 image
            image_bytes = base64.b64decode(image_data)
            result = await analyzer.analyze_image(image_bytes)
        else:
            # Process from URL
            result = await analyzer.analyze_image_url(image_url)
        
        return {
            "status": "completed",
            "output": result
        }
        
    except Exception as e:
        logger.error(f"Request processing failed: {e}")
        return {
            "error": str(e),
            "status": "failed"
        }

def handler(job: Dict[str, Any]) -> Dict[str, Any]:
    """RunPod serverless handler function"""
    # Initialize services if not already done
    if not analyzer:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        success = loop.run_until_complete(initialize_services())
        if not success:
            return {
                "error": "Service initialization failed",
                "status": "failed"
            }
    
    # Process request
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(process_request(job))

# Preload models on container start for FlashBoot
if __name__ == "__main__":
    import preload_models
    asyncio.run(preload_models.preload_models())
EOF

# Set RunPod handler
ENV RUNPOD_HANDLER_PATH="/app/handler.py"

# FlashBoot optimization environment variables
ENV FLASHBOOT_ENABLED=true \
    MODEL_CACHE_ENABLED=true \
    PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512" \
    CUDA_LAUNCH_BLOCKING=0 \
    TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0"

# Expose port for health checks
EXPOSE 8080

# Health check for container readiness
HEALTHCHECK --interval=10s --timeout=5s --start-period=30s --retries=3 \
    CMD python -c "import torch; assert torch.cuda.is_available()" || exit 1

# RunPod expects the handler to be available
CMD ["python", "-u", "/app/handler.py"]